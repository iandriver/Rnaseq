The RNA-Seq Pipeline

What it Does:
--------------------------------------------------------------------------------

The purpose of the RNA-Seq pipeline is two-fold.  

The first purpose is to provide a tool to biologists that simplifies the process
of analyzing the large amount of RNA-Seq data generated by such experiments.
Such analysis is typically performed in a step-wise fasion, with each step
implemented by a separate script.  This pipeline framework allows such scripts
to be grouped together into one batch job (suitable for running on a desktop or
cluster), with appropriate parameters for each script in place.

The second purpose of the pipeline is to allow more sophisticated users to alter
the contents of the pipeline (scripts, parameters, inputs) in order to compare
the performance of various tools used within the pipeline.  A mechanism for
recording the performamce of each step is provided, as well as provenance for
each intermediate data file generated.


How it works:
--------------------------------------------------------------------------------
The basic operation of the pipeline software is to convert a group of
text-based configuration files (YAML format) into a shell script that can be run
on a desktop computer or cluster.  The configuration files separate information
in a modular way.  Different files describe the data to be analyzed,
each step of the pipeline, how the steps fit together to form the overall
pipeline, and a final file that defines "global" values (independent of any
specific pipeline).  The pipeline software then assembles the data in these
configuration files into a runnable shell script and launches the job on the
desired hardware configuration.


How to install it:
--------------------------------------------------------------------------------
There are currently several ways to obtain the sourcecode.  If you have git on
your computer, use this command: 

git clone git://github.com/phonybone/Rnaseq.git

If you don't have access to git, email me at vcassen@systemsbiology.org and I
will make a tar-ball available to you.

Git also requires the following python packages to be installed on your system:
- evoque
- qpy
- yaml

These can be install using the python module intaller "easy_install".  If you
don't have root/administrator privileges on your system, you can define the
PYTHONPATH environment variable to specify a custom installation location.
Google "python easy_install" for more information.



Globals configuration:
Before you can run the pipeline tool, you must edit the config file
'config/rnaseq.conf.yml' (relative to the installation directory).  This is a
yaml file.  Some of the sections are marked "Do not edit", and the tool will
break if the values in those sections are changed.  Of the remaining sections, 
most of the values should be self-explanatory.  All paths are relative to the
installation directory unless starting with '/'.

Some entries that must be configured are:
- root_dir: synonomous with the installation directory
- blat_indexes, bowtie_indexes: the directory in which indexes for these
  programs can be found.
- path: the PATH environment variable used in the shell scripts that are
  generated.
- db:db_name: the location of the sqlite3 database to use for provenance
- qsub: the values in this section refer to the Sun Grid Engine cluster to be
  used.  If you have access to such a cluster, edit the values in this section
  as appropriate.  For help email me at vcassen@systemsbiology.org.


How to operate it:
--------------------------------------------------------------------------------
Simple version - analysis without tweaking the pipeline steps or parameters:

The pipeline tool is a python script named 'rnaseq', located in the bin/
directory of the application package (ie, wherever it was installed).  You can
add this directory to you PATH, put a link from /usr/local/bin to this
directory, or otherwise make the rnaseq script findable to your system.  

The basic command to run a pipeline is this:

python bin/rnaseq <cmd> -p <pipeline> -r <readset_file>

where <pipeline> is the name of pre-defined pipeline (see below) and
<readset_file> is the name of a file that defines the data to be analyzed (also
see below).

A list of all the command-line parameters can be displayed by invoking the -h
flag:

python bin/rnaseq -h

which yields:

Usage: bin/rnaseq <cmd> [-p <pipeline>] [-r <readset_file>] [options]

Options:
  -h, --help            show this help message and exit
  --aligner=RNASEQ__ALIGNER
                        specify aligner
  --align_suffix=RNASEQ__ALIGN_SUFFIX
                        internal use
  --fq_cmd=RNASEQ__FQ_CMD
                        internal use
  --cluster             execute operations on a cluster (requires additional
                        config settings)
  -c CONFIG_FILE, --config=CONFIG_FILE
                        specify alternative config file
  -f, --force           force execution of pipelines and steps even if targets
                        are up to date
  -n, --no_run          supress actuall running
  -p PIPELINE_NAME, --pipeline=PIPELINE_NAME
                        pipeline name
  -r READSET_FILE, --readset=READSET_FILE
                        readset filename



Defining the data set:

To launch a pipeline job that performs RNA-Seq analysis, it is necessary to
specify the data to be analyzed and the pipeline with which to analyze it.

The data is defined in a config file called a "readset".  The data can be in
various formats (.SAM, Fasta/Fastq, Illumina reads, etc); it is expected that
the pipeline configuration will specify steps to perform any data format
conversions as necessary.  (Currently, pipelines are static entities that cannot
vary their behaviour depending the data they analyze or other run-time factors.
This may change in the future).

The absolute minimum data a readset needs is the location of the file(s)
containing the data.  However, most analysis will require additional metadata
such as the length of the reads and so forth.  A typical readset configuration
file might look like this:

reads_file: /users/vcassen/links/samples/sandbox/s_6_export.1K.txt
description: this is a sample readset
org: mouse
readlen: 75
working_dir: rnaseq_wf

Each line contains a single key-value pair, separated by a colon character.  The
exact key-value pairs required are defined by the particular pipeline's needs;
these values are referenced later on by the pipeline configuration files.

Note: the 'reads_file' value may contain a fileglob, eg "s_?_export.txt"; if so,
the glob is expanded under normal unix-posix rules and a separate pipeline
instance is created for each value in the glob.

fixme:
This small configuration file must reside in a specific directory.  The
directory is templates/readsets/<name_of_readset>.syml.  In the future this
requirement should vanish.


Specifying the pipeline:

The second piece of information you must specify is which pipeline you actually
wish to use.  Like readsets, pipelines are defined in config files, but their
structure is more complicated.  If you are using a pre-packaged pipeline, you
do not need to worry about the config structure of a pipeline; you simply
specify the pipeline by name using the '-p' flag (see examples below).

Fixme: add a command to show the list of available pipelines.

Running a pipeline: simple example

To actually run a pipeline, you issue a command like the following:

python bin/rnaseq run -p <pipeline> -r <readset>  [--cluster]

As mentioned above, the '-p' flag specifies the name of the pipeline you'd like
to run, and the '-r' flag specifies the name of the readset that defines the
data.  The 'run' portion of the command tells the rnaseq to run the pipeline;
there are other commands it understands as well (see below).  If you wish to run
your job on the cluster, include the optional --cluster flag.


Advanced version - changing internals:

Warning: in it's current state, modifying pipelines is a complicated task that
is best done with the author's help.  It is intended that this process become
easier with time, and the current state of the software should be considered a
rough alpha version.

In order to modify pipelines, it is necessary to understand how they're
defined.  In short, there are two sets of configuration files needed to define a
pipeline.  The first set consists of a single config file that defines each step
in the pipeline and assigns parameters for that step such as inputs and
outputs.  The second set of config files define the steps themselves.  

Pipeline config files are stored in the templates/pipeline directory.  Each file
must end with the '.syml' extension.  The file consists of a header section and
a section for each step.  Since these are YAML files, indentation is important:
Indenting by a constant number of spaces defines a hash.  The section header
corresponds to the name of the step, and the values contained therein are
specific to the step.  For example:


filterQuality:
  input: ${ID}.${align_suffix}
  output: ${ID}.qual_OK.${align_suffix}
  filtered: ${ID}.bowtie_qual_BAD.${align_suffix}
  args: -v -f ${align_suffix}


In this example, four fields are defined for the step named 'filterQuality', whose
purpose is to remove reads with low quality scores from the readset.

- input: This defines the input file.  It accesses the 'reads_file' field of the
  readset object.  The "${}" notation indicates a variable ("readset"), and the
  ['reads_file'] part indicates a sub-field of the variable.
- output: This defines the output file, the name of which in this case being
  composed of two parts.  The first part, "${ID}", is a special variable defined
  by the pipeline.  It is based on the name of the input file (as defined by the
  readset, and in this case the same as the input) and the working directory,
  which can be defined in the readset or the pipeline.  See below for more info.
  "${align_suffix}" is also a special variable defined by the pipeline software
  and based on the "-aligner" command line option.
- filtered: This entry specifies the location of the filtered reads, which can
  then be used as the input for a different step later on.
- args: these are args that will be supplied to the actual program, which is
  defined by the step of the same name.

Below is the base config file that defines the filterQuality step itself:

name: filterQuality
description: Remove sequences with low quality scores
usage: '%(interpreter)s %(exe)s %(args)s -i %(input)s -o %(output)s -b %(filtered)s'
interpreter: perl
exe: filterQuality.pl



Each step may use several config files in it's definition as follows.  A "base"
file is found in templates/step/<step_name>.syml.  It can contain the following fields:

- name: the name of the step

- description: a description of what the step does

- usage: this is a python format string that is used to create the actual
  command that will be issued.  It uses the python "hash format" version of the
  '%' operator, which means the '%s' expansions are named and refer to keys in a
  dict object.  All of the keys must present in the final dict object used to
  expand the format string, but not all the keys must be defined in the step
  itself; they may also be defined by the pipeline, the global config dict, or
  by other entities.

- exe: the name of the script or executable that implements the step.  This may
  be either an absolute or relative file system path; if relative, it is
  relative to the bin/ directory under the installation directory.  Links are
  allowed. 

- interpreter: if 'exe', above, is a script, than this field must contain the
  name of the interpreter that runs the script (eg perl, python, etc).

- args: If there are invariant args to this step, they can be defined here.  For
  args that vary from pipeline to pipeline, define the args in the pipeline
  section of the step instead.

- sh_template: some steps require more than one shell-level command to run.  If
  a step defines a sh_template: entry, it refers to a snippet of sh code in
  which the step command may be imbedded.  The template itself must reside in
  the templates/sh_template subdirectory (relative to the installation
  directory). Within the template, the ${sh_cmd} variable is expanded to the
  usage-exanded string mentioned above.  See bowtie_filter.tmpl as an example.

- prototype: Some steps are simply variants of a base step.  For example, many
  steps use an aligner (currently either blat or bowtie, but others could be
  defined).  If a step defines a prototype field, a step of with the given name
  is loaded and it's values are added to that of the "current" step's.

- Other values may be defined by step.syml files, and the values then referenced
  from pipeline as needed.


In the filterQuality step, above, note how the 'usage' fields makes reference to
entities defined by both the base step configuration (eg 'interpreter' or
'exe'), and to entities defined by the step's section in the pipeline (eg
'input', 'output', and 'args').  The pipeline tool will combine all this
information, plus information defined in the readset config, to produce a
command line entry that looks something like this: 

perl filterQuality.pl -v -f fq -i /users/vcassen/links/samples/sandbox/rnaseq_wf/s_6_export.1K.txt.fq -o /users/vcassen/links/samples/sandbox/rnaseq_wf/s_6_export.1K.txt.qual_OK.fq -b /users/vcassen/links/samples/sandbox/rnaseq_wf/s_6_export.1K.txt.bowtie_qual_BAD.fq

Special variables:

There are a number of special variables available to steps.  They are listed
below, but are likely to change with future versions of the software.  

${ID}: This variable is a combination of the 'working_dir' field, defined either
in the readset or pipeline, and the reads_file defined by the readset.  The
purpose of this variable is to simplify the construction of inputs and outputs
in step sections of the pipeline configuration file, and to allow different
subdirectories to be specified as output destinations of different pipeline runs
(so that the output of one run doesn't overwrite the output of a second run
using the same input data).

${align_suffix}: This is currently set to either 'fq' or 'fa', depending on the
value of the command-line parameter '--aligner'.  If '--aligner' == 'blat',
${align_suffix} is set to 'fa'; if '--aligner' is set to 'bowtie',
${align_suffix} is set to 'fq'.  This value can be overwritten by the pipeline
or by a command-line parameter '--align_suffix'

A note about YAML file structure:
Indentation bla bla bla

How to create a new pipeline or modify an existing one:

1. Edit the pipeline.syml file
Each pipeline must have it's own configuration file.  The file is divided into
two parts, a header section and a step definition section.

The header section consists of "key: value" pairs.  There are two required
entries: "name:" and "stepnames:", but there may be as many key/value pairs as
needed.  The "name:" value can be anything, but should be kept fairly short.  It
must also be unique.  The "stepnames:" entry must list all the steps of the
pipeline by name, with each step name separated by a comma and/or whitespace
(all on one line, however).

The pipeline configuration file may refer to variables defined in the global
configuration file (including command-line options) or variables defined by the
readset.  Additionally, the special variable ${ID} is defined.

1a. Edit the list of steps (stepnames)

1b. Edit the section for each step
Each step must have a section in the pipeline configuration file.  The step
section must define variables that the step itself leaves undefined (see below);
that is, values that are pipeline-specific.  Typically this includes the names
of the input and output files, files that are created as side-effects, and
specific command-line arguments to the program that the step executes.
Specifically, all of the entities mentioned in the step's "usage:" must be
defined either by the step itself or by the step's section in the pipeline
configuration file.

The list of steps mentioned in the "stepnames:" entry of the header must match
in a 1-to-1 way with the list of step sections.

2. Create a step:
Steps are defined by a set of small configuration files.  At a minimum the set
contains a file located in the templates/step/ directory with a name comprised
of the stepname and the suffix ".syml".  For example, a step to implement a
program named "foo" might exist in templates/step/foo.syml.

2a. Create the step/stepname.syml file

The step.syml file must define a few fields:
- name: This must correspond to the name of the step listed in the pipeline
configuration file, and must also be the same as the basename of the .syml
file. 
- description: This should be a short (one line) description of what the step
does. 
- usage: This field is used to create the shell command that runs the step.  It
is a python format string, which means that it will be used as the left-hand
operand of the the python '%' string format operator.  In practice what this
means is that substrings of the form "%(field)s" will be replaced with the value
defined by "field".  For example, "%(name)s" would be replaced with the value of
the step's name as defined by the "name:" field.
- exe: This entry must contain the file system location of the program to run as
an absolute path, or a path relative to and of the directories listed in the
global configuration "path" entry (in the rnaseq section).
- interpreter: if the program is an interpreted script and does not have the
execution bit set in the file permissions, you can specify the needed
interpreter using this field.  As with the "exe:" field, the "interpreter:"
field must be "findable" in the directories specified by the global "path:"
value.  This field is not required for non-interpreted executables.

The step may define and reference (in the usage string) any other values it
needs.  The main requirement is that every field referenced by the usage field
be defined either by the step or in the step's section within the pipeline
configuration file.  Missing fields will result in a run-time error and be
reported to the user.

In addition, there are a few other fields that allow more complicated behaviour
of steps.  They are:

2b. If the step needs more than one command line, create the
    sh_template/stepname.syml file

Some steps logically require more than just the running of their program.  For
example, a step might want to "clean up" after itself by removing an unneeded
file generated by the program.  While it would be possible to define a second
step to perform this task, it makes sense to include the operations within same
step since they're logically connected.  In order to implement this, a step may
define 

2c. Sometimes it is useful to create variations of a step that share some
information (such as the executable) but vary other parts, (perhaps the
command-line parameters used).  In this case, it is possible to base a new step
on an existing one, using the existing step as a "prototype".  When this is
done, the two steps combine all their defined values when the usage string is
constructed.  When both prototype and derived steps define the same fields,
those of the derived step overwrite the prototype.  This can be used to
establish default values in the prototype that may be superceded by the derived
step.  Additionally, the prototype may leave certain fields blank to be later
filled in by the derived step (with an error raised if the field remains
undefined).

If the step is derived from a prototype, indicate it by using a "prototype:"
entry in derived step's .syml file.  The value of the entry will be the name of the step
from which the new step will be prototyped.


A Complete Example:

The readset.syml:
reads_file: /users/vcassen/links/samples/sandbox/s_?_export.1M.txt
description: this is a sample readset
org: mouse
readlen: 75
working_dir: rnaseq_wf

Note the file glob in the reads_file:, and the working_dir: entry which will
direct all output to a subdirectory with the name rnaseq_wf (created in the same
directory as the reads_files)

A short pipeline.syml:

name: pipeline de Juan
stepnames: header export2fq filterQuality filterVectors footer

header:
  name: header

export2fq:
  input: ${readset['reads_file']}
  output: ${ID}.${rnaseq['align_suffix']}
  args: ${rnaseq['fq_cmd']} 

filterQuality:
  input: ${ID}.${rnaseq['align_suffix']}
  output: ${ID}.qual_OK.${rnaseq['align_suffix']}
  filtered: ${ID}.bowtie_qual_BAD.${rnaseq['align_suffix']}
  args: -v -f ${rnaseq['align_suffix']}

filterVectors:
  input: ${ID}.${rnaseq['align_suffix']}
  output: ${ID}.vector_OK.${rnaseq['align_suffix']}
  filtered: ${ID}.bowtie_vector_BAD.sam
  psl: ${ID}.blat_vector_BAD.psl

footer:
  name: footer

This pipeline has five steps: a header step, three actual processing steps, and
a footer step.  

In the top section, two key-value pairs are defined.  Note that the value for
the stepnames: entry exactly corresponds to the five step sections that follow.

The header and footer steps are required for every pipeline (fixme).  

The export2fq step translates an Illumina export file into a different format
using a perl script.  

The input to this script is the reads_file defined by the readset (in the case
that the reads_file contains a file glob, a loop is created that iterates over
all the files that match the file glob and a script is generated for each file.
In this scenario, the 'reads_file' entry in the readset object is set to the
current reads_file).

The output uses the special ${ID} variable created from the current reads_file
and the working_dir (which can be set in either the readset or the pipeline
config file).

Two other variable substitutions are made in the export2fq step:
${rnaseq['align_suffix']} and ${rnaseq['fq_cmd']}.  These actually represent a
"hack" which may or may not remain in the program.  The hack is that, based on
the value of the command-line parameter "--aligner", these two extra entries are
added to the 'rnaseq' section of the global configuration file.  This allows the
user to specify the alignment tool to use without changing the pipeline as a
whole.  In this case, 'align_suffix' is set to 'fq', and 'fq_cmd' is set to the
proper value for that program ('solexa2fastq').  This is not a sustainable
strategy and providing a better solution is high on the "TODO" list.

The export2fq.syml looks like this:

name: export2fq
description: convert an Illumina export file to FASTQ format
prototype: fq_all2std

It is based on a prototype, 'fq_all2std'.   This is the fq_all2std.syml file:

name: fq_all2std
usage: '%(interpreter)s %(exe)s %(args)s %(input)s %(output)s'
interpreter: perl
exe: fq_all2std.pl

As you can see, between the pipeline's export2fq: section and the two step .syml
files, all the attributes referenced in the usage format string are accounted for.


The next step is 'filterQuality'.  This step removes low-quality reads from the
reads_file.  You can see from the pipeline section for this step and the
previous step that the input to this step is the same as the output from the
previous step.

The filterQuality.syml file looks like this:

name: filterQuality
description: Remove sequences with low quality scores
usage: '%(interpreter)s %(exe)s %(args)s -i %(input)s -o %(output)s -b %(filtered)s'
interpreter: perl
exe: filterQuality.pl

It is fairly straightforward.  Notice the additional field "filtered" in the
usage string, and it's definition in the step's section in the pipeline file.


The final step (aside from the footer step) is filterVectors:

name: filterVectors
description: filter vectors
ewbt: UniVec_Core
blat_index: ${pipeline['blat_index']}/solexa_primers.2bit
prototype: ${config['rnaseq']['aligner']}_filter

This is a prototype-based step.  The prototype that the step is based on depends
on the value of the aligner as specified on the command line; it can be either
'bowtie' or 'blat'.  The actual step name is either 'bowtie_filter' or
'blat_filter'.  blat_filter.syml looks like this:

name: blat
description: align the reads to the ref. geneome (using blat)
usage: '%(exe)s %(blat_index)s %(input)s %(args)s %(psl)s'
exe: blat
sh_template: blat_filter.tmpl
args: -fastMap -q=DNA -t=DNA

Note that some fields are needed by bowtie and others by blat, but both must be
defined regardless of which aligner is used.

