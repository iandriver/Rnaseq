The RNA-Seq Pipeline

What it Does:
--------------------------------------------------------------------------------

The purpose of the RNA-Seq pipeline is two-fold.  

The first purpose is to provide a tool to biologists that simplifies the process
of analyzing the large amount of RNA-Seq data generated by such experiments.
Such analysis is typically performed in a step-wise fasion, with each step
implemented by a separate script.  This pipeline framework allows such scripts
to be grouped together into one batch job (suitable for running on a desktop or
cluster), with appropriate parameters for each script in place.

The second purpose of the pipeline is to allow more sophisticated users to alter
the contents of the pipeline (scripts, parameters, inputs) in order to compare
the performance of various tools used within the pipeline.  A mechanism for
recording the performamce of each step is provided, as well as provenance for
each intermediate data file generated.


How it works:
--------------------------------------------------------------------------------
The basic operation of the pipeline software is to convert a group of
text-based configuration files (YAML format) into a shell script that can be run
on a desktop computer or cluster.  The configuration files separate information
in a modular way, with different files that describe the data to be analyzed,
each step of the pipeline, how the steps fit together to form the overall
pipeline, and a final file that defines "global" values (independent of any
specific pipeline).  The pipeline software then assembles the data in these
configuration files into a runnable shell script and launches the job on the
desired hardware configuration.


How to install it:
There are currently several ways to obtain the sourcecode.  If you have git on
your computer, use this command: 

git clone git://github.com/phonybone/Rnaseq.git

If you don't have access to git, email me at vcassen@systemsbiology.org and I
will make a tar-ball available to you.

Git also requires the following python packages to be installed on your system:
- evoque
- qpy
- yaml

These can be install using the python module intaller "easy_install".  If you
don't have root/administrator privileges on your system, you can define the
PYTHONPATH environment variable to specify a custom installation location.
Google "python easy_install" for more information.



Globals configuration:
Before you can run the pipeline tool, you must edit in the config file
'config/rnaseq.conf.yml' (relative to the installation directory).  This is a
yaml file.  Some of the sections are marked "Do not edit", and the tool will
break if the values in those sections are changed.  Of the remaining sections, 
most of the values should be self-explanatory.  All paths are relative to the
installation directory unless starting with '/'.

- root_dir: synonomous with the installation directory
- blat_indexes, bowtie_indexes: the directory in which indexes for these
  programs can be found.
- path: the PATH environment variable used in the shell scripts that are
  generated.
- db:db_name: the location of the sqlite3 database to use for provenance
- qsub: the values in this section refer to the Sun Grid Engine cluster to be
  used.  If you have access to such a cluster, edit the values in this section
  as appropriate.  For help email me at vcassen@systemsbiology.org.


How to operate it:
--------------------------------------------------------------------------------
Simple version - analysis without tweaking the pipeline steps or parameters:

The pipeline tool is a python script named 'rnaseq', located in the bin/
directory of the application package (ie, wherever it was installed).  You can
add this directory to you PATH, put a link from /usr/local/bin to this
directory, or otherwise make the rnaseq script findable to your system.  See
below for a list of command-line parameters and examples.

Defining the data set:

To launch a pipeline job that performs RNA-Seq analysis, it is necessary to
specify the data to be analyzed and the pipeline with which to analyze it.

The data is defined in a config file called a "readset".  The data can be in
various formats (.SAM, Fasta/Fastq, Illumina reads, etc); it is expected that
the pipeline configuration will specify steps to perform any data format
conversions as necessary.  (Currently, pipelines are static entities that cannot
vary their behaviour depending the data they analyze or other run-time factors.
This may change in the future).

The absolute minimum data a readset needs is the location of the file(s)
containing the data.  However, most analysis will require additional metadata
such as the length of the reads and so forth.  A typical readset configuration
file might look like this:

reads_file: /users/vcassen/links/samples/sandbox/s_6_export.1K.txt
description: this is a sample readset
org: mouse
readlen: 75
working_dir: rnaseq_wf

Each line contains a single key-value pair, separated by a colon character.  The
exact key-value pairs required are defined by the particular pipeline's needs;
these values are referenced later on by the pipeline configuration files.

Note: the 'reads_file' value may contain a fileglob, eg "s_?_export.txt"; if so,
the glob is expanded under normal unix-posix rules and a separate pipeline
instance is created for each value in the glob.

fixme:
This small configuration file must reside in a specific directory.  The
directory is templates/readsets/<name_of_readset>.syml.  In the future this
requirement should vanish.


Specifying the pipeline:

The second piece of information you must specify is which pipeline you actually
wish to use.  Like readsets, pipelines are defined in config files, but their
structure is more complicated.  If you are using a pre-packaged pipeline, you
do not need to worry about the config structure of a pipeline; you simply
specify the pipeline by name using the '-p' flag (see examples below).

Fixme: add a command to show the list of available pipelines.

Running a pipeline: simple example

To actually run a pipeline, you issue a command like the following:

python bin/rnaseq run -p <pipeline> -r <readset>  [--cluster]

As mentioned above, the '-p' flag specifies the name of the pipeline you'd like
to run, and the '-r' flag specifies the name of the readset that defines the
data.  The 'run' portion of the command tells the rnaseq to run the pipeline;
there are other commands it understands as well (see below).  If you wish to run
your job on the cluster, include the optional --cluster flag.


Advanced version - changing internals:

Warning: in it's current state, modifying pipelines is a complicated task that
is best done with the author's help.  It is intended that this process become
easier with time, and the current state of the software should be considered a
rough alpha version.

In order to modify pipelines, it is necessary to understand how they're
defined.  In short, there are two sets of configuration files needed to define a
pipeline.  The first set consists of a single config file that defines each step
in the pipeline and assigns parameters for that step such as inputs and
outputs.  The second set of config files define the steps themselves.  

Pipeline config files are stored in the templates/pipeline directory.  Each file
must end with the '.syml' extension.  The file consists of a header section and
a section for each step.  Since these are YAML files, indentation is important:
Indenting by a constant number of spaces defines a hash.  The section header
corresponds to the name of the step, and the values contained therein are
specific to the step.  For example:


filterQuality:
  input: ${ID}.${align_suffix}
  output: ${ID}.qual_OK.${align_suffix}
  filtered: ${ID}.bowtie_qual_BAD.${align_suffix}
  args: -v -f ${align_suffix}


In this example, four fields are defined for the step named 'filterQuality', whose
purpose is to remove reads with low quality scores from the readset.

- input: This defines the input file.  It accesses the 'reads_file' field of the
  readset object.  The "${}" notation indicates a variable ("readset"), and the
  ['reads_file'] part indicates a sub-field of the variable.
- output: This defines the output file, the name of which in this case being
  composed of two parts.  The first part, "${ID}", is a special variable defined
  by the pipeline.  It is based on the name of the input file (as defined by the
  readset, and in this case the same as the input) and the working directory,
  which can be defined in the readset or the pipeline.  See below for more info.
  "${align_suffix}" is also a special variable defined by the pipeline software
  and based on the "-aligner" command line option.
- filtered: This entry specifies the location of the filtered reads, which can
  then be used as the input for a different step later on.
- args: these are args that will be supplied to the actual program, which is
  defined by the step of the same name.

Below is the base config file that defines the filterQuality step itself:

name: filterQuality
description: Remove sequences with low quality scores
usage: '%(interpreter)s %(exe)s %(args)s -i %(input)s -o %(output)s -b %(filtered)s'
interpreter: perl
exe: filterQuality.pl



Each step may use several config files in it's definition as follows.  A "base"
file is found in templates/step/<step_name>.syml.  It can contain the following fields:
- name: the name of the step
- description: a description of what the step does
- usage: this is a python format string that is used to create the actual
  command that will be issued.  It uses the python "hash format" version of the
  '%' operator, which means the '%s' expansions are named and refer to keys in a
  dict object.  All of the keys must present in the final dict object used to
  expand the format string, but not all the keys must be defined in the step
  itself; they may also be defined by the pipeline, the global config dict, or
  by other entities.
- exe: the name of the script or executable that implements the step.  This may
  be either an absolute or relative file system path; if relative, it is
  relative to the bin/ directory under the installation directory.  Links are
  allowed. 
- interpreter: if 'exe', above, is a script, than this field must contain the
  name of the interpreter that runs the script (eg perl, python, etc).
- args: If there are invariant args to this step, they can be defined here.  For
  args that vary from pipeline to pipeline, define the args in the pipeline
  section of the step instead.
- sh_template: some steps require more than one shell-level command to run.  If
  a step defines a sh_template: entry, it refers to a snippet of sh code in
  which the step command may be imbedded.  The template itself must reside in
  the templates/sh_template subdirectory (relative to the installation
  directory). Within the template, the ${sh_cmd} variable is expanded to the
  usage-exanded string mentioned above.  See bowtie_filter.tmpl as an example.
- prototype: Some steps are simply variants of a base step.  For example, many
  steps use an aligner (currently either blat or bowtie, but others could be
  defined).  If a step defines a prototype field, a step of with the given name
  is loaded and it's values are added to that of the "current" step's.

- Other values may be defined by step.syml files, and the values then referenced
  from pipeline as needed.


In the filterQuality step, above, note how the 'usage' fields makes reference to
entities defined by both the base step configuration (eg 'interpreter' or
'exe'), and to entities defined by the step's section in the pipeline (eg
'input', 'output', and 'args').  The pipeline tool will combine all this
information, plus information defined in the readset config, to produce a
command line entry that looks something like this: 

perl filterQuality.pl -v -f fq -i /users/vcassen/links/samples/sandbox/rnaseq_wf/s_6_export.1K.txt.fq -o /users/vcassen/links/samples/sandbox/rnaseq_wf/s_6_export.1K.txt.qual_OK.fq -b /users/vcassen/links/samples/sandbox/rnaseq_wf/s_6_export.1K.txt.bowtie_qual_BAD.fq

Special variables:

There are a number of special variables available to steps.  They are listed
below, but are likely to change with future versions of the software.  

${ID}: This variable is a combination of the 'working_dir' field, defined either
in the readset or pipeline, and the reads_file defined by the readset.  The
purpose of this variable is to simplify the construction of inputs and outputs
in step sections of the pipeline configuration file, and to allow different
subdirectories to be specified as output destinations of different pipeline runs
(so that the output of one run doesn't overwrite the output of a second run
using the same input data).

${align_suffix}: This is currently set to either 'fq' or 'fa', depending on the
value of the command-line parameter '--aligner'.  If '--aligner' == 'blat',
${align_suffix} is set to 'fa'; if '--aligner' is set to 'bowtie',
${align_suffix} is set to 'fq'.  This value can be overwritten by the pipeline
or by a command-line parameter '--align_suffix'

